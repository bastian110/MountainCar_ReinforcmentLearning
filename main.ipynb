{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "n_bins = ( 200 , 200 )\n",
    "lower_bounds = [ env.observation_space.low[0], env.observation_space.high[0] ]\n",
    "upper_bounds = [ env.observation_space.high[1], env.observation_space.low[1] ]\n",
    "\n",
    "def discretizer( position, velocity ):\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[position, velocity]])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.43705848 0.0\n",
      "(122, 20)\n",
      "0.7539862169692985\n"
     ]
    }
   ],
   "source": [
    "print(*env.reset()[0])\n",
    "print(discretizer(*env.reset()[0]))\n",
    "print(random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space 3\n",
      "State Space Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2040, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "env.reset() # reset environment to a new, random state\n",
    "env_discretized = discretizer(*env.reset()[0])\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space.n))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "q_table = np.zeros([env_discretized[0]*env_discretized[1],env.action_space.n])\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \u001b[39m# Explore action space\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m#print('ok')\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     state \u001b[39m=\u001b[39m discretizer(\u001b[39m*\u001b[39;49mstate)\n\u001b[1;32m     29\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table[state]) \u001b[39m# Exploit learned values\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#(q_table)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mdiscretizer\u001b[0;34m(position, velocity)\u001b[0m\n\u001b[1;32m      8\u001b[0m est \u001b[39m=\u001b[39m KBinsDiscretizer(n_bins\u001b[39m=\u001b[39mn_bins, encode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mordinal\u001b[39m\u001b[39m'\u001b[39m, strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m est\u001b[39m.\u001b[39mfit([lower_bounds, upper_bounds ])\n\u001b[0;32m---> 10\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m,est\u001b[39m.\u001b[39;49mtransform([[position, velocity]])[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:345\u001b[0m, in \u001b[0;36mKBinsDiscretizer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m# check input and attribute dtypes\u001b[39;00m\n\u001b[1;32m    344\u001b[0m dtype \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 345\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    347\u001b[0m bin_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbin_edges_\n\u001b[1;32m    348\u001b[0m \u001b[39mfor\u001b[39;00m jj \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Xt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/sklearn/utils/validation.py:752\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39m# store whether originally we wanted numeric dtype\u001b[39;00m\n\u001b[1;32m    750\u001b[0m dtype_numeric \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 752\u001b[0m dtype_orig \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(array, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    753\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(dtype_orig, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    754\u001b[0m     \u001b[39m# not a data type (e.g. a column named dtype in a pandas DataFrame)\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 10000001):\n",
    "    \n",
    "    k=0\n",
    "    state = discretizer(*env.reset()[0])\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while k<=200:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "            #print('ok')\n",
    "        else:\n",
    "            state = discretizer(*state)\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        #(q_table)\n",
    "        next_state, reward, done, info, test = env.step(action) \n",
    "        state = discretizer(*state)\n",
    "        #print('state',state)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[discretizer(*next_state)])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -1:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        k+=1\n",
    "    #print(\"couo\")\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table[state])\n\u001b[0;32m---> 12\u001b[0m     next_state, reward, done, info, test \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action) \n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     15\u001b[0m         penalties \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:147\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m (position, velocity)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:264\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    265\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    267\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\").env\n",
    "for _ in range(episodes):\n",
    "    state = discretizer(*env.reset()[0])\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info, test = env.step(action) \n",
    "\n",
    "        if reward == -1:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose nbin?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50864f6fedbad3869f0e347d491007b310d129d28e52e7dc5c02e32e9fc16d50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
