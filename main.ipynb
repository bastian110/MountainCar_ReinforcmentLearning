{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "n_bins = ( 20 , 20 )\n",
    "lower_bounds = [ env.observation_space.low[0], env.observation_space.high[0] ]\n",
    "upper_bounds = [ env.observation_space.high[1], env.observation_space.low[1] ]\n",
    "\n",
    "def discretizer( position, velocity ):\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[position, velocity]])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.55472517 0.0\n",
      "(10, 2)\n",
      "0.3028033932143471\n"
     ]
    }
   ],
   "source": [
    "print(*env.reset()[0])\n",
    "print(discretizer(*env.reset()[0]))\n",
    "print(random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space 3\n",
      "State Space Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "env.reset() # reset environment to a new, random state\n",
    "env_discretized = discretizer(*env.reset()[0])\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space.n))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "q_table = np.zeros([env_discretized[0]*env_discretized[1],env.action_space.n])\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4900\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "max_velocity = 0\n",
    "\n",
    "for i in range(1, 5000):\n",
    "    \n",
    "    k=0\n",
    "    state = discretizer(*env.reset()[0])\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while k<=200:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "            #print('ok')\n",
    "        else:\n",
    "            state = discretizer(*state)\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        #(q_table)\n",
    "        next_state, reward, done, info, test = env.step(action) \n",
    "        if next_state[1] > max_velocity:\n",
    "            max_velocity = next_state[1]\n",
    "            reward += 20\n",
    "\n",
    "        state = discretizer(*state)\n",
    "        #print('state',state)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[discretizer(*next_state)])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -1:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        k+=1\n",
    "    #print(\"couo\")\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table[state])\n\u001b[0;32m---> 12\u001b[0m     next_state, reward, done, info, test \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action) \n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     15\u001b[0m         penalties \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:147\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m (position, velocity)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:234\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [(carwidth \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m-\u001b[39mcarwidth \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m0\u001b[39m)]:\n\u001b[1;32m    231\u001b[0m     c \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mVector2(c)\u001b[39m.\u001b[39mrotate_rad(math\u001b[39m.\u001b[39mcos(\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m pos))\n\u001b[1;32m    232\u001b[0m     wheel \u001b[39m=\u001b[39m (\n\u001b[1;32m    233\u001b[0m         \u001b[39mint\u001b[39m(c[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m (pos \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_position) \u001b[39m*\u001b[39m scale),\n\u001b[0;32m--> 234\u001b[0m         \u001b[39mint\u001b[39m(c[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m clearance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_height(pos) \u001b[39m*\u001b[39m scale),\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m     gfxdraw\u001b[39m.\u001b[39maacircle(\n\u001b[1;32m    238\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, wheel[\u001b[39m0\u001b[39m], wheel[\u001b[39m1\u001b[39m], \u001b[39mint\u001b[39m(carheight \u001b[39m/\u001b[39m \u001b[39m2.5\u001b[39m), (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m     gfxdraw\u001b[39m.\u001b[39mfilled_circle(\n\u001b[1;32m    241\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, wheel[\u001b[39m0\u001b[39m], wheel[\u001b[39m1\u001b[39m], \u001b[39mint\u001b[39m(carheight \u001b[39m/\u001b[39m \u001b[39m2.5\u001b[39m), (\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m    242\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ReinforcmentLearning_project1/.venv/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:167\u001b[0m, in \u001b[0;36mMountainCarEnv._height\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_height\u001b[39m(\u001b[39mself\u001b[39m, xs):\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49msin(\u001b[39m3\u001b[39;49m \u001b[39m*\u001b[39;49m xs) \u001b[39m*\u001b[39;49m \u001b[39m0.45\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m0.55\u001b[39;49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\").env\n",
    "for _ in range(episodes):\n",
    "    state = discretizer(*env.reset()[0])\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info, test = env.step(action) \n",
    "\n",
    "        if reward == -1:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose nbin?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50864f6fedbad3869f0e347d491007b310d129d28e52e7dc5c02e32e9fc16d50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
